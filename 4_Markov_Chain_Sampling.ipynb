{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import emcee\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from pyazr import azure2\n",
    "\n",
    "from multiprocess import Pool\n",
    "\n",
    "# Restrict processes to one thread only\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# emcee variables\n",
    "nsteps = 10000 # How many steps should each walker take?\n",
    "nthin  = 1     # How often should the walker save a step?\n",
    "nprocs = 20    # How many Python processes do you want to allocate?\n",
    "\n",
    "# Define the data labels (in AZURE2 order)\n",
    "labels = [\"Meyer et al. (1976) - 84.3 deg\",\n",
    "          \"Meyer et al. (1976) - 114.5 deg\",\n",
    "          \"Meyer et al. (1976) - 144.1 deg\", \n",
    "          \"LUNA HPGe (2023)\", \n",
    "          \"LUNA BGO (2023)\",\n",
    "          \"Felsenkeller (2023)\",\n",
    "          \"ATOMKI (2023)\",\n",
    "          \"Notre Dame (2023) - 0 deg\",\n",
    "          \"Notre Dame (2023) - 55 deg\",\n",
    "          \"Burtebaev et al. (2008)\",\n",
    "          \"Lamb et al. (1957)\",\n",
    "          \"Bailey et al. (1950)\",\n",
    "          \"Vogl et al. (1963)\",\n",
    "          \"Rolfs et al. (1974) - 0 deg\",\n",
    "          \"Rolfs et al. (1974) - 90 deg\"]\n",
    "\n",
    "# Normalization parameter map : { index : (value, error) } (in AZURE2 order)\n",
    "norms = { 0:  (1, 0.05),   1:  (1, 0.05),   2: (1, 0.05),  \n",
    "          3:  (1, 0.069),  4:  (1, 0.079),  5: (1, 0.1),  \n",
    "          6:  (1, 0.1),    7:  (1, 0.1),    8: (1, 0.1),  \n",
    "          9:  (1, 0.1),    10: (1, 0.0),   11: (1, 0.0), \n",
    "          12: (1, 0.0),    13: (1, 0.0),   14: (1, 0.0) }\n",
    "\n",
    "# Define the parameters prior distributions\n",
    "priors = [\n",
    "    stats.norm(1.63,0.12),\n",
    "\n",
    "    stats.uniform(2.30, 0.10), stats.uniform(0, 100000), stats.uniform(-10, 20),\n",
    "    stats.uniform(-1000000,2000000),\n",
    "\n",
    "    stats.uniform(3.45, 0.10), stats.uniform(0, 100000), stats.uniform(-10, 20), stats.uniform(-10, 20),\n",
    "    stats.uniform(-1000000,2000000), stats.uniform(-1000000,2000000),\n",
    "\n",
    "    stats.uniform(3.50, 0.10), stats.uniform(0, 100000),\n",
    "    \n",
    "    stats.norm(1.0,0.05),\n",
    "    stats.norm(1.0,0.05),\n",
    "    stats.norm(1.0,0.05),\n",
    "    stats.norm(1.0,0.069),\n",
    "    stats.norm(1.0,0.079),\n",
    "    stats.norm(1.0,0.10),\n",
    "    stats.norm(1.0,0.10),\n",
    "    stats.norm(1.0,0.10),\n",
    "    stats.norm(1.0,0.10),\n",
    "    stats.norm(1.0,0.10),\n",
    "    stats.uniform(0, 100),\n",
    "    stats.uniform(0, 100),\n",
    "    stats.uniform(0, 100),\n",
    "    stats.uniform(0, 100),\n",
    "    stats.uniform(0, 100)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the .azr file and set the external capture file to speed up the calculation\n",
    "azr = azure2('12c_pg.azr', nprocs=nprocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the initial values from AZURE2\n",
    "theta0 = azr.params\n",
    "ntheta = len(theta0)\n",
    "\n",
    "# Now we add the normalizations\n",
    "for norm in norms.items( ):\n",
    "    theta0 = np.concatenate( (theta0, [norm[1][0]]) )\n",
    "\n",
    "# We'll read the data from the output file since it's already in the center-of-mass frame\n",
    "y = azr.cross\n",
    "yerr = azr.cross_err\n",
    "ndata = sum( len(segment) for segment in y )\n",
    "\n",
    "# Number of Walkers = 2 x Number of Parameters\n",
    "nw = 2 * ntheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior log probability\n",
    "def lnPi( theta ):\n",
    "    return np.sum([pi.logpdf(t) for (pi, t) in zip(priors, theta)])\n",
    "\n",
    "# Log likelihood\n",
    "def lnl( theta, proc=0 ):\n",
    "    mu = azr.calculate( theta[:ntheta], proc=proc )\n",
    "    lnl = np.sum( -0.5 * np.log(2 * np.pi* pow(yerr, 2) ) - 0.5 * pow((y - mu) / yerr, 2) )\n",
    "    return lnl\n",
    "\n",
    "# Posterior log probability\n",
    "def lnP( theta ):\n",
    "    lnpi = lnPi( theta )\n",
    "    if not np.isfinite( lnpi ): return -np.inf\n",
    "    return lnl( theta ) + lnpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing initial parameters for the walkers\n",
    "p0 = np.zeros((nw, ntheta))\n",
    "mean, std = np.loadtxt('minuit/best_lnl.txt')[:,0], np.loadtxt('minuit/best_lnl.txt')[:,1]\n",
    "for i in range(nw): p0[i, :] = stats.norm(mean, std).rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [6:20:17<00:00, 22.82s/it]   \n"
     ]
    }
   ],
   "source": [
    "backend = emcee.backends.HDFBackend('results/mcmc/samples.h5') \n",
    "backend.reset(nw, ntheta)\n",
    "\n",
    "with Pool(processes=nprocs) as pool:\n",
    "    sampler = emcee.EnsembleSampler( nw, ntheta, lnP, pool=pool, backend=backend ) \n",
    "    state = sampler.run_mcmc( p0, nsteps, thin_by=nthin, progress=True, tune=True )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
